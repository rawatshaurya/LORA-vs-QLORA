# ReasonTune ğŸ”  
**Reasoning-Style Fine-Tuning with LoRA vs QLoRA**

This project explores **parameter-efficient fine-tuning (PEFT)** for large language models by comparing **LoRA** and **QLoRA** on a reasoning task (**CommonsenseQA**).  
The goal is to study the **accuracy vs memory trade-off** when adapting instruction-tuned LLMs under different GPU constraints.

---

## ğŸš€ Project Highlights

- Fine-tuned **Qwen/Qwen2.5-1.5B-Instruct**
- Compared **LoRA (FP16)** vs **QLoRA (4-bit NF4)**
- Evaluated reasoning performance using **Exact Match**
- Tracked **peak GPU memory usage**
- Designed to be **Windows + single-GPU friendly**

---

## ğŸ§  What This Project Demonstrates

- How **LoRA** injects low-rank adapters into attention layers  
- How **QLoRA** enables training under tight VRAM budgets via 4-bit quantization  
- Practical challenges of mixed precision and AMP on Windows  
- Trade-offs between **model quality** and **hardware efficiency**

This is a **real-world PEFT comparison**, not a toy experiment.

---

## ğŸ“Š Results (Quick Eval, n = 200)

**Dataset:** `tau/commonsense_qa`  
**Train samples:** 8,000  
**Sequence length:** 1024  

| Method | Base Precision | Exact Match | Peak VRAM (Reserved) | Train Time |
|------|----------------|-------------|----------------------|------------|
| **LoRA** | FP16 | **0.825** | **6546 MB** | 1498 s |
| **QLoRA** | INT4 (NF4) + FP16 compute | 0.745 | **2086 MB** | 2106 s |

### Key Takeaways

- **QLoRA reduced peak VRAM usage by ~68%**
- This came with an **~8 point drop in accuracy**
- **LoRA** is preferable when accuracy matters
- **QLoRA** is ideal when GPU memory is the bottleneck

![Accuracy vs Memory](outputs/accuracy_vs_memory.png)

---

## ğŸ—ï¸ Project Structure

```text
ReasonTune/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ train_csqa.yaml
â”‚   â”œâ”€â”€ train_csqa_qlora.yaml
â”‚   â””â”€â”€ eval.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_lora.py
â”‚   â”œâ”€â”€ eval.py
â”‚   â”œâ”€â”€ infer.py
â”‚   â”œâ”€â”€ plot_accuracy_vs_memory.py
â”‚   â”œâ”€â”€ data.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ csqa_run_1/
â”‚   â”œâ”€â”€ csqa_qlora_run_1/
â”‚   â””â”€â”€ accuracy_vs_memory.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create and activate a virtual environment
```bash
python -m venv .venv
.venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

> **Note:** CUDA-enabled PyTorch is required for GPU training.

---

## ğŸ‹ï¸ Training

### ğŸ”¹ LoRA (FP16)
```bash
cp configs/train_csqa.yaml configs/train.yaml
python -m src.train_lora
```

### ğŸ”¹ QLoRA (4-bit)
```bash
cp configs/train_csqa_qlora.yaml configs/train.yaml
python -m src.train_lora
```

---

**Built as a hands-on PEFT study for real-world LLM fine-tuning.**
